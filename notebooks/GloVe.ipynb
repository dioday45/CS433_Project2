{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/jdidio/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "import os\n",
    "import sys\n",
    "os.path.abspath(os.curdir)\n",
    "os.chdir(\"..\")\n",
    "ML_FOLDER_PATH = os.path.abspath(os.curdir)\n",
    "sys.path.append(ML_FOLDER_PATH)\n",
    "import src.helpers as hlp\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn import svm\n",
    "\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pos = pd.read_table(\"data/train_pos.txt\", header=None, names=['tweet'], dtype=str,on_bad_lines='skip')\n",
    "t_pos['label'] = 1\n",
    "t_neg = pd.read_table(\"data/train_neg.txt\", header=None, names=['tweet'], dtype=str,on_bad_lines='skip')\n",
    "t_neg['label'] = -1\n",
    "df = pd.concat((t_pos,t_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196970/196970 [00:00<00:00, 1213473.33it/s]\n",
      "100%|██████████| 196970/196970 [00:00<00:00, 297048.24it/s]\n",
      "100%|██████████| 196970/196970 [00:00<00:00, 263525.04it/s]\n",
      "100%|██████████| 196970/196970 [00:00<00:00, 293575.52it/s]\n",
      "100%|██████████| 196970/196970 [00:00<00:00, 303091.76it/s]\n",
      "100%|██████████| 196970/196970 [00:01<00:00, 147172.78it/s]\n",
      "100%|██████████| 196970/196970 [00:00<00:00, 421872.92it/s]\n",
      "100%|██████████| 196970/196970 [00:00<00:00, 495001.20it/s]\n",
      "100%|██████████| 196970/196970 [00:00<00:00, 555541.62it/s]\n",
      "100%|██████████| 196970/196970 [00:00<00:00, 463867.95it/s]\n",
      "100%|██████████| 196970/196970 [00:03<00:00, 50746.19it/s]\n",
      "100%|██████████| 173235/173235 [00:06<00:00, 28474.58it/s]\n"
     ]
    }
   ],
   "source": [
    "df = hlp.preprocess_data(df)\n",
    "df['tweet'] = df['tweet'].progress_apply(lambda s: tokenizer.tokenize(s))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet embedding using pretrained glove\n",
    "Here we download a pretrained GloVe model which is trained on Twitter data and has a dimension of 100 per words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "print(list(api.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_twitter_100 = api.load(\"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_embedding(model, tweet):\n",
    "    vec = np.zeros((len(tweet), 100))\n",
    "    for count, w in enumerate(tweet):\n",
    "        try:\n",
    "            w_vec = model.get_vector(w)\n",
    "            vec[count] = w_vec\n",
    "        except:\n",
    "            pass\n",
    "    vectors = np.array(vec)\n",
    "    return vectors.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 173235/173235 [00:03<00:00, 51418.91it/s]\n"
     ]
    }
   ],
   "source": [
    "df['tweet'] = df['tweet'].progress_apply(lambda s: tweet_embedding(glove_twitter_100, s))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(df.tweet.tolist())\n",
    "y = df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_split = ShuffleSplit(n_splits=5,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]..................................................................................................................................*.*.**\n",
      "optimization finished, #iter = 1325\n",
      "Objective value = -112552.527883\n",
      "nSV = 142733\n",
      "[LibLinear]...................................................................................................................................**.***.*\n",
      "optimization finished, #iter = 1331\n",
      "Objective value = -112507.711996\n",
      "nSV = 142817\n",
      "[LibLinear]...................................................................................................................................**.*\n",
      "optimization finished, #iter = 1323\n",
      "Objective value = -112547.092859\n",
      "nSV = 142840\n",
      "[LibLinear]...................................................................................................................................*..............................................................*.***\n",
      "optimization finished, #iter = 1946\n",
      "Objective value = -112571.687873\n",
      "nSV = 142802\n",
      "[LibLinear]....................................................................................................................................*..........................................................**.**\n",
      "optimization finished, #iter = 1917\n",
      "Objective value = -112773.697224\n",
      "nSV = 142915\n"
     ]
    }
   ],
   "source": [
    "svm_model = svm.LinearSVC(verbose=1, max_iter=20000)\n",
    "svm_score = cross_val_score(svm_model, X, y, cv=s_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM mean accuracy = 0.7365\n",
      "SVM standard deviation accuracy = 0.0032\n"
     ]
    }
   ],
   "source": [
    "print(f'SVM mean accuracy = {svm_score.mean():.4f}')\n",
    "print(f'SVM standard deviation accuracy = {svm_score.std():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov  4 2022, 11:11:31) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c2cef518e44103d2e87bbc7bdbad3c849c59c4ad82abf34e0b5c0b1400204ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
