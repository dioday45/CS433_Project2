{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\nimport torch\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import EarlyStoppingCallback","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-08T11:21:11.850564Z","iopub.execute_input":"2022-12-08T11:21:11.851148Z","iopub.status.idle":"2022-12-08T11:21:11.859706Z","shell.execute_reply.started":"2022-12-08T11:21:11.851078Z","shell.execute_reply":"2022-12-08T11:21:11.858513Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport wandb\ntqdm.pandas()\n\nif torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-12-08T11:21:11.861712Z","iopub.execute_input":"2022-12-08T11:21:11.862804Z","iopub.status.idle":"2022-12-08T11:21:11.878073Z","shell.execute_reply.started":"2022-12-08T11:21:11.862763Z","shell.execute_reply":"2022-12-08T11:21:11.876862Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"There are 2 GPU(s) available.\nDevice name: Tesla T4\n","output_type":"stream"}]},{"cell_type":"code","source":" # Load data and set labels\ndata_complaint = pd.read_table(\"../input/cs433tweets/train_neg.txt\", header=None, names=['tweet'], dtype=str,on_bad_lines='skip')\ndata_complaint['label'] = 0\ndata_non_complaint = pd.read_table(\"../input/cs433tweets/train_pos.txt\", header=None, names=['tweet'], dtype=str,on_bad_lines='skip')\ndata_non_complaint['label'] = 1\n\n# Concatenate complaining and non-complaining data\ndata = pd.concat((data_complaint, data_non_complaint)).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T11:21:11.879521Z","iopub.execute_input":"2022-12-08T11:21:11.880720Z","iopub.status.idle":"2022-12-08T11:21:12.288965Z","shell.execute_reply.started":"2022-12-08T11:21:11.880680Z","shell.execute_reply":"2022-12-08T11:21:12.287900Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Define pretrained tokenizer and model\nmodel_name = \"bert-base-uncased\"\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T11:21:12.291451Z","iopub.execute_input":"2022-12-08T11:21:12.291788Z","iopub.status.idle":"2022-12-08T11:21:34.841236Z","shell.execute_reply.started":"2022-12-08T11:21:12.291762Z","shell.execute_reply":"2022-12-08T11:21:34.840150Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5b9783073f44a2fa7ad57f9d2f2d10d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e82d05f4e684572b401bb20389d2204"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a163e5e09ad24e65b4bdacebf6e883a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a431d5369244a2f96c04d97e783d3fe"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"X = data.tweet.values.tolist()\ny = data.label.values.tolist()\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=2022)\n\nX_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\nX_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T11:21:34.846789Z","iopub.execute_input":"2022-12-08T11:21:34.848669Z","iopub.status.idle":"2022-12-08T11:23:34.331561Z","shell.execute_reply.started":"2022-12-08T11:21:34.848615Z","shell.execute_reply":"2022-12-08T11:23:34.330526Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.labels:\n            item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])","metadata":{"execution":{"iopub.status.busy":"2022-12-08T11:23:34.332973Z","iopub.execute_input":"2022-12-08T11:23:34.334278Z","iopub.status.idle":"2022-12-08T11:23:34.342650Z","shell.execute_reply.started":"2022-12-08T11:23:34.334238Z","shell.execute_reply":"2022-12-08T11:23:34.341709Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset(X_train_tokenized, y_train)\nval_dataset = Dataset(X_val_tokenized, y_val)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T11:23:34.344015Z","iopub.execute_input":"2022-12-08T11:23:34.344981Z","iopub.status.idle":"2022-12-08T11:23:34.355721Z","shell.execute_reply.started":"2022-12-08T11:23:34.344943Z","shell.execute_reply":"2022-12-08T11:23:34.354512Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(p):\n    pred, labels = p\n    pred = np.argmax(pred, axis=1)\n\n    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n    recall = recall_score(y_true=labels, y_pred=pred)\n    precision = precision_score(y_true=labels, y_pred=pred)\n    f1 = f1_score(y_true=labels, y_pred=pred)\n\n    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}","metadata":{"execution":{"iopub.status.busy":"2022-12-08T11:23:34.358392Z","iopub.execute_input":"2022-12-08T11:23:34.359385Z","iopub.status.idle":"2022-12-08T11:23:34.368432Z","shell.execute_reply.started":"2022-12-08T11:23:34.359349Z","shell.execute_reply":"2022-12-08T11:23:34.367268Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=\"output\",\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    seed=2022,\n    load_best_model_at_end=True,\n    report_to='wandb')\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T11:23:34.369796Z","iopub.execute_input":"2022-12-08T11:23:34.370300Z","iopub.status.idle":"2022-12-08T11:23:39.947582Z","shell.execute_reply.started":"2022-12-08T11:23:34.370262Z","shell.execute_reply":"2022-12-08T11:23:39.946542Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Train pre-trained model\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2022-12-08T11:23:39.951110Z","iopub.execute_input":"2022-12-08T11:23:39.951517Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 177273\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 33240\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='501' max='33240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  501/33240 06:52 < 7:31:02, 1.21 it/s, Epoch 0.05/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='561' max='1232' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 561/1232 03:09 < 03:46, 2.96 it/s]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 19697\n  Batch size = 16\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}