{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "os.path.abspath(os.curdir)\n",
    "os.chdir(\"..\")\n",
    "ML_FOLDER_PATH = os.path.abspath(os.curdir)\n",
    "sys.path.append(ML_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/jdidio/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import src.helpers as hlp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from gensim.models import Word2Vec, doc2vec\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pos = pd.read_table(\"data/train_pos.txt\", header=None, names=['tweet'], dtype=str,on_bad_lines='skip')\n",
    "t_pos['label'] = 1\n",
    "t_neg = pd.read_table(\"data/train_neg.txt\", header=None, names=['tweet'], dtype=str,on_bad_lines='skip')\n",
    "t_neg['label'] = -1\n",
    "df = pd.concat((t_pos,t_neg), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196970/196970 [00:00<00:00, 283933.44it/s]\n",
      "100%|██████████| 196970/196970 [00:00<00:00, 264959.03it/s]\n",
      "100%|██████████| 196970/196970 [00:00<00:00, 293068.25it/s]\n",
      "100%|██████████| 196970/196970 [00:00<00:00, 303712.95it/s]\n",
      "100%|██████████| 196970/196970 [00:01<00:00, 149172.15it/s]\n",
      "100%|██████████| 196970/196970 [00:00<00:00, 1273147.93it/s]\n",
      "100%|██████████| 196970/196970 [00:00<00:00, 425000.70it/s]\n",
      "100%|██████████| 196970/196970 [00:00<00:00, 499732.37it/s]\n",
      "100%|██████████| 196970/196970 [00:00<00:00, 565010.83it/s]\n",
      "100%|██████████| 196970/196970 [00:00<00:00, 209223.34it/s]\n",
      "100%|██████████| 196970/196970 [00:03<00:00, 54581.86it/s]\n",
      "100%|██████████| 158945/158945 [00:04<00:00, 36660.20it/s]\n"
     ]
    }
   ],
   "source": [
    "df['tweet'] = df['tweet'].progress_apply(lambda x: hlp.remove_stopwords(x))\n",
    "df['tweet'] = df['tweet'].progress_apply(lambda x: hlp.remove_punct(x))\n",
    "df['tweet'] = df['tweet'].progress_apply(lambda x: hlp.add_space(x))\n",
    "df['tweet'] = df['tweet'].progress_apply(lambda x: hlp.remove_white_space(x))\n",
    "df['tweet'] = df['tweet'].progress_apply(lambda x: hlp.remove_words_digits(x))\n",
    "df['tweet'] = df['tweet'].progress_apply(lambda x: hlp.to_lower(x))\n",
    "df['tweet'] = df['tweet'].progress_apply(lambda x: hlp.remove_specific_words(x))\n",
    "df['tweet'] = df['tweet'].progress_apply(lambda x: hlp.remove_repeating_char(x))\n",
    "df['tweet'] = df['tweet'].progress_apply(lambda x: hlp.remove_single_char(x))\n",
    "df['tweet'] = df['tweet'].progress_apply(lambda x: hlp.remove_non_english_words(x))\n",
    "df['tweet'] = df['tweet'].progress_apply(lambda x: hlp.lemmatize(x))\n",
    "df = df[df['tweet'] != '']\n",
    "df = df.drop_duplicates()\n",
    "df.reset_index(inplace=True)\n",
    "df['tweet'] = df['tweet'].progress_apply(lambda x: hlp.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and train Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/jdidio/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "text = 'hello '\n",
    "print(hlp.tokenize(text))\n",
    "\n",
    "import nltk \n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are you doing\n"
     ]
    }
   ],
   "source": [
    "sent = \"hellooo how are you doing\"\n",
    "\n",
    "\n",
    "sent = \" \".join(w for w in nltk.wordpunct_tokenize(sent) if w.lower() in words or not w.isalpha())\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8164272, 8960540)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = Word2Vec(df['tweet'], vector_size=50, window=3, min_count=2, sg=1, seed=16)\n",
    "w2v.train(df['tweet'], total_examples= len(df['tweet']), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/158945 [00:00<07:05, 373.86it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'get_vector'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z2/kbk820zs4qqcblm94ncvb7fc0000gn/T/ipykernel_62735/1857705527.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m                 \u001b[0;31m# on the df using our wrapper (which provides bar updating)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4431\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4432\u001b[0m         \"\"\"\n\u001b[0;32m-> 4433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4435\u001b[0m     def _reduce(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1144\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m                     \u001b[0;31m# take a fast or slow code path; so stop when t.total==t.n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                 \u001b[0;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/z2/kbk820zs4qqcblm94ncvb7fc0000gn/T/ipykernel_62735/1857705527.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'get_vector'"
     ]
    }
   ],
   "source": [
    "df['tweet'] = df['tweet'].progress_apply(lambda x: w2v(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_embedding(model, tweet):\n",
    "    vec = np.zeros((len(tweet), 50))\n",
    "    for count, w in enumerate(tweet):\n",
    "        try:\n",
    "            w_vec = model.wv.get_vector(w)\n",
    "            vec[count] = w_vec\n",
    "        except:\n",
    "            continue\n",
    "    vectors = np.array(vec)\n",
    "    return vectors.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].apply(lambda x: tweet_embedding(w2v, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(df.tweet.tolist())\n",
    "y = df.label\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.10, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]....................................................................................................\n",
      "optimization finished, #iter = 1000\n",
      "\n",
      "WARNING: reaching max number of iterations\n",
      "Using -s 2 may be faster (also see FAQ)\n",
      "\n",
      "Objective value = -151404.664229\n",
      "nSV = 155874\n",
      "SVM training accuracy = 0.5827\n",
      "SVM val accuracy = 0.5837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jdidio/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "LSVC = LinearSVC(verbose=1)\n",
    "LSVC.fit(X_train, y_train)\n",
    "print(f'SVM training accuracy = {LSVC.score(X_train, y_train):.4f}')\n",
    "print(f'SVM val accuracy = {LSVC.score(X_val, y_val):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pos = pd.read_table(\"data/train_pos.txt\", header=None, names=['tweet'], dtype=str,on_bad_lines='skip')\n",
    "t_pos['label'] = 1\n",
    "t_neg = pd.read_table(\"data/train_neg.txt\", header=None, names=['tweet'], dtype=str,on_bad_lines='skip')\n",
    "t_neg['label'] = -1\n",
    "df = pd.concat((t_pos,t_neg), ignore_index=True)\n",
    "df['tweet'] = df['tweet'].apply(lambda x: hlp.remove_stopwords(x))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: hlp.remove_punct(x))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: hlp.add_space(x))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: hlp.remove_white_space(x))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: hlp.remove_words_digits(x))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: hlp.to_lower(x))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: hlp.remove_specific_words(x))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: hlp.remove_repeating_char(x))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: hlp.remove_single_char(x))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: hlp.lemmatize(x))\n",
    "df = df[df['tweet'] != '']\n",
    "df = df.drop_duplicates()\n",
    "df.reset_index(inplace=True)\n",
    "df['tweet'] = df['tweet'].apply(lambda x: hlp.tokenize(x))\n",
    "df_tagged = df.apply(lambda r: doc2vec.TaggedDocument(words=r.tweet, tags=[r.label]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = doc2vec.Doc2Vec(dm=1, vector_size=50, negative=5, hs=0, min_count=2, sample = 0)\n",
    "model_dbow.build_vocab(df_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.train(df_tagged, total_examples=len(df_tagged), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].apply(lambda x: model_dbow.infer_vector(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(df.tweet.tolist())\n",
    "y = df.label\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.10, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear].................................................*\n",
      "optimization finished, #iter = 495\n",
      "Objective value = -116026.816099\n",
      "nSV = 142506\n",
      "SVM training accuracy = 0.7185\n",
      "SVM val accuracy = 0.7190\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "LSVC = LinearSVC(verbose=1)\n",
    "LSVC.fit(X_train, y_train)\n",
    "print(f'SVM training accuracy = {LSVC.score(X_train, y_train):.4f}')\n",
    "print(f'SVM val accuracy = {LSVC.score(X_val, y_val):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM training accuracy = 0.7267\n",
      "SVM val accuracy = 0.7184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaboost = AdaBoostClassifier(n_estimators=100)\n",
    "adaboost.fit(X_train, y_train)\n",
    "print(f'SVM training accuracy = {adaboost.score(X_train, y_train):.4f}')\n",
    "print(f'SVM val accuracy = {adaboost.score(X_val, y_val):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c2cef518e44103d2e87bbc7bdbad3c849c59c4ad82abf34e0b5c0b1400204ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
