{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1069,"status":"ok","timestamp":1670176585591,"user":{"displayName":"T casti","userId":"16165362413792083600"},"user_tz":-60},"id":"Q3-v4i-jfJLi"},"outputs":[],"source":["# Install the transformers library\n","#!pip install transformers\n","#!pip install datasets\n","#!pip install wandb\n","#!pip install pytorch-lightning"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":843,"status":"ok","timestamp":1670176586773,"user":{"displayName":"T casti","userId":"16165362413792083600"},"user_tz":-60},"id":"uTOQG0Rtzv5O","outputId":"3147430b-d571-4bc6-e7a8-bb067448a8e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sun Dec  4 17:56:26 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   54C    P0    28W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17911,"status":"ok","timestamp":1670176604674,"user":{"displayName":"T casti","userId":"16165362413792083600"},"user_tz":-60},"id":"FH9psCYTfNMW","outputId":"f348d4b6-b799-49b8-dca6-2c64a7dd9cd5"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["device=device(type='cuda', index=0)\n"]}],"source":["import torch\n","from torch import nn, optim\n","import torch.nn.functional as F\n","import pytorch_lightning as pl\n","from torch.utils.data import Dataset, DataLoader\n","import wandb\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","from datetime import timedelta\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","import helpers as hlp\n","from sklearn.model_selection import train_test_split\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","from transformers import (\n","    AutoTokenizer, AutoModelForSequenceClassification,\n","    TrainingArguments, Trainer, DataCollatorWithPadding\n",")\n","RANDOM_SEED = 42\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f'{device=}')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21755,"status":"ok","timestamp":1670176626306,"user":{"displayName":"T casti","userId":"16165362413792083600"},"user_tz":-60},"id":"3bsYglYHfRyx","outputId":"571a19ea-124e-44b2-fc57-547f8144c0f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["(177273, 2) (19697, 2)\n"]}],"source":["t_pos = pd.read_table(\"train_pos.txt\", header=None, names=['tweet'], dtype=str,on_bad_lines='skip')\n","t_pos['label'] = 1\n","t_neg = pd.read_table(\"train_neg.txt\", header=None, names=['tweet'], dtype=str,on_bad_lines='skip')\n","t_neg['label'] = 0\n","df = pd.concat((t_pos,t_neg), ignore_index=True)\n","#df['tweet'] = df['tweet'].apply(lambda x: hlp.remove_stopwords(x))\n","#df['tweet'] = df['tweet'].apply(lambda x: hlp.remove_punct(x))\n","df['tweet'] = df['tweet'].apply(lambda x: hlp.add_space(x))\n","df['tweet'] = df['tweet'].apply(lambda x: hlp.remove_white_space(x))\n","#df['tweet'] = df['tweet'].apply(lambda x: hlp.remove_words_digits(x))\n","df['tweet'] = df['tweet'].apply(lambda x: hlp.to_lower(x))\n","#df['tweet'] = df['tweet'].apply(lambda x: hlp.remove_specific_words(x))\n","df['tweet'] = df['tweet'].apply(lambda x: hlp.remove_repeating_char(x))\n","#df['tweet'] = df['tweet'].apply(lambda x: hlp.remove_single_char(x))\n","df['tweet'] = df['tweet'].apply(lambda x: hlp.lemmatize(x))\n","train_df, val_df = train_test_split(df,test_size=.1,random_state=42)\n","print(train_df.shape,val_df.shape)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":115,"status":"ok","timestamp":1670176626311,"user":{"displayName":"T casti","userId":"16165362413792083600"},"user_tz":-60},"id":"ZkZ2yg65uqlF"},"outputs":[],"source":["class TweetDataset(Dataset):\n","  \"\"\"\n","  torch dataset class specific for our project\n","  \"\"\"\n","  def __init__(self,tweets_df, tokenizer, max_len):\n","    super().__init__()\n","    self.tweets = tweets_df.tweet.to_numpy()\n","    self.targets = tweets_df.label.to_numpy()\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","  \n","  def __len__(self):\n","    return len(self.tweets)\n","  \n","  def __getitem__(self, item):\n","    \"\"\"\n","    encodes the input with the bert tokenizer\n","    \"\"\"\n","    tweets = str(self.tweets[item])\n","    targets = self.targets[item]\n","    encoding = self.tokenizer.encode_plus(\n","      tweets,\n","      add_special_tokens=True,\n","      max_length=self.max_len,\n","      return_token_type_ids=False,\n","      pad_to_max_length=True,\n","      return_attention_mask=True,\n","      return_tensors='pt',\n","    )\n","    return {\n","      'tweet_text': tweets,\n","      'input_ids': encoding['input_ids'].flatten(),\n","      'attention_mask': encoding['attention_mask'].flatten(),\n","      'targets': torch.tensor(targets, dtype=torch.long)\n","    }\n","  \n","class TweetDataModule(pl.LightningDataModule):\n","\n","  def __init__(self,train_dataset, val_dataset, batch_size=8):\n","    super().__init__()\n","    self.train_dataset = train_dataset\n","    self.val_dataset = val_dataset\n","    self.batch_size = batch_size\n","\n","  def train_dataloader(self):\n","      return DataLoader(self.train_dataset,self.batch_size)\n","\n","  def val_dataloader(self):\n","      return DataLoader(self.val_dataset, self.batch_size)\n","\n","\n","class SentimentClassifier(pl.LightningModule):\n","  \"\"\"\n","  Sentiment classifier model. adds a feed-forward NN to bert-base-uncased.\n","  \"\"\"\n","  def __init__(self, model_name, lr = 2e-5, adam_eps=1e-8, weight_decay=0., warmup_steps=0):\n","    super(SentimentClassifier, self).__init__()\n","    self.save_hyperparameters()\n","    self.bert = BertModel.from_pretrained(model_name)\n","    self.drop = nn.Dropout(p=0.3)\n","    self.out = nn.Linear(self.bert.config.hidden_size, 2)\n","    self.loss_fn = nn.CrossEntropyLoss().to(device)\n","    self.lr = lr\n","    self.adam_eps = adam_eps\n","    self.warmup_steps = warmup_steps\n","\n","  def forward(self, input_ids, attention_mask):\n","    pooled_output = self.bert(\n","      input_ids=input_ids,\n","      attention_mask=attention_mask\n","    )[1]\n","    output = self.drop(pooled_output)\n","    return F.softmax(self.out(output), dim=1)\n","\n","  def training_step(self, batch, batch_idx):\n","      input_ids = batch['input_ids']\n","      attention_mask = batch['attention_mask']\n","      targets = batch['targets']\n","      logits = self.forward(input_ids,attention_mask)\n","\n","      loss = self.loss_fn(logits,targets)\n","      self.log('train/loss', loss)\n","      return loss\n","\n","  def validation_step(self, batch, batch_idx):\n","      input_ids = batch['input_ids']\n","      attention_mask = batch['attention_mask']\n","      targets = batch['targets']\n","      logits = self.forward(input_ids,attention_mask)\n","      preds = torch.argmax(logits,dim=1)\n","      acc = (preds == targets).sum()/len(targets)\n","      loss = self.loss_fn(logits,targets)\n","      self.log_dict({'val/loss': loss, 'val/acc': acc})\n","      return {'val_loss':loss, 'val_accuracy': acc}\n","\n","  def predict_step(self, batch, batch_idx):\n","      input_ids = batch['input_ids']\n","      attention_mask = batch['attention_mask']\n","      logits = self.forward(input_ids,attention_mask)\n","      preds = torch.argmax(logits,dim=1)\n","      return logits, preds\n","  \n","  def configure_optimizers(self):\n","      optimizer = torch.optim.Adam(\n","          self.parameters(),\n","          lr=self.lr,\n","          eps=self.adam_eps\n","          )\n","      \n","      scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=self.warmup_steps,\n","            num_training_steps=self.trainer.estimated_stepping_batches,\n","        )\n","      scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n","      return [optimizer], [scheduler]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":694,"referenced_widgets":["b8e51e0768f4464082657a635acfaeb4","dbf1c0a737e14eae821bb920c075ff40","295089d4cd2f44a19a65e5a7db89726d","e00c539f56dd4a06a976a483e1aef78a","13fab2a81b324a2a8c071160223b330f","5e44b32d5fb04ab1a254f3c40238b077","f4029d0de91e4744a644a158601c2804","868bf2b6d7704d0a9fbb6a95e104dcc1","fe8fa82ece694ed48a0532ff04d34c59","4b725bd619b64c51bfafacca14b12f3b","4954a08985774034a19b73dfcb550f5d","40a50d8140d445bcaa482cea8415dc95","6552ae43a46543b8aec6cf9f8a44760c","d8104f8bda9247cc8bfbda69372dd082","83dbb6105c1b4566b34912beb7d6a81e","56c53c712f6943daa30e114e9e8b6576","6d3d69f5e2c64c5c8c820882fad26a40","51258d65aabb4e6a8d7a176c8e58a293","a909c7e0f61641ebad29ec04e0e275ab","acb1e4bd9ad74a729b0d5c2f9220e0cf","e8ca78a03fde4be4b9d1e06509f214dd","e6a5948d6a8a4586a778823823bffdec","2dd5c9ef87f44dd89b4bbd739695fa55","ccde8064e8504460aabe9c5b9c10d893","e01e9b9b6f374433945244e839e8d306","c3c55c7e8d124694a54b8a12c92f714c","3ceb27d19523438ba36be11f4bfb2f5c","09bd16cc9d6e4c77bd98ab82bc93abf1","1ef645a992b24073add4dd1a8b6fedb3","44459ba678f54c6191c614d007f7c2bd","0cd4b4a0a38046f5a20c9ef542433f8f","cda2b46b5af44aa2893e1e2f8803f882","3d843c1ea5cd488eb3f42c75307a5c9d"]},"id":"el9tQ6e7z91Z","outputId":"57127942-f0b9-47af-fdf2-eb4081c020fc"},"outputs":[{"name":"stderr","output_type":"stream","text":["ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtcastigl\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20221204_175708-24ebbotk</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/tcastigl/TweetSentimentClassifier/runs/24ebbotk\" target=\"_blank\">epochs1_small_dataset</a></strong> to <a href=\"https://wandb.ai/tcastigl/TweetSentimentClassifier\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO:lightning_lite.utilities.seed:Global seed set to 42\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loggers/wandb.py:396: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n","  rank_zero_warn(\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory /content/logs/checkpoints exists and is not empty.\n","  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n","INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","INFO:pytorch_lightning.utilities.rank_zero:Loading `train_dataloader` to estimate number of stepping batches.\n","INFO:pytorch_lightning.callbacks.model_summary:\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | bert    | BertModel        | 109 M \n","1 | drop    | Dropout          | 0     \n","2 | out     | Linear           | 1.5 K \n","3 | loss_fn | CrossEntropyLoss | 0     \n","---------------------------------------------\n","109 M     Trainable params\n","0         Non-trainable params\n","109 M     Total params\n","437.935   Total estimated model params size (MB)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b8e51e0768f4464082657a635acfaeb4","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40a50d8140d445bcaa482cea8415dc95","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2dd5c9ef87f44dd89b4bbd739695fa55","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["wandb.init(project=\"TweetSentimentClassifier\", entity=\"tcastigl\", name='epochs1_small_dataset')\n","pl.seed_everything(42)\n","BATCH_SIZE=64\n","TOKENIZER_MAX_LEN=128\n","PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n","tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","\n","dm = TweetDataModule(\n","    TweetDataset(train_df,tokenizer, TOKENIZER_MAX_LEN),\n","    TweetDataset(val_df,tokenizer, TOKENIZER_MAX_LEN),\n","    batch_size=BATCH_SIZE\n","  )\n","wandb.config.update({'batch_size': BATCH_SIZE,\n","                     'dataset':  'small',\n","                     'tokenizer_max_len': TOKENIZER_MAX_LEN})\n","\n","#model = SentimentClassifier(PRE_TRAINED_MODEL_NAME, weight_decay=.2)\n","model = SentimentClassifier.load_from_checkpoint(\"logs/checkpoints/last.ckpt\")\n","model = model.to(device)\n","\n","logger = pl.loggers.WandbLogger(\n","    save_dir='logs',\n","    project='bertClassifier',\n","    name='try',\n","    resume='must'\n",")\n","checkpoint_callback = pl.callbacks.ModelCheckpoint(\n","    dirpath=Path('logs/', 'checkpoints'),\n","    filename=\"best-model-epoch={epoch:02d}\",\n","    monitor=\"val/loss\",\n","    train_time_interval=timedelta(minutes=20),\n","    save_on_train_epoch_end=True,\n","    # auto_insert_metric_name=False,\n","    save_top_k=1,\n","    save_last=True,\n","    mode=\"min\",\n",")\n","trainer = pl.Trainer(\n","    max_epochs=2,\n","    logger=logger,\n","    callbacks= [checkpoint_callback],\n","    log_every_n_steps=50,\n","    val_check_interval=500,\n","    enable_progress_bar=True,\n","    accelerator='gpu',\n","    devices=1\n",")\n","\n","\n","trainer.fit(model, datamodule=dm)\n","wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SxU7c9V5fgxs"},"outputs":[],"source":["test_df = pd.read_table(\"test_data.txt\", header=None, dtype=str, names=['tweet'], on_bad_lines='skip')\n","for i, tweet in enumerate(test_df.tweet):\n","  test_df.loc[i, 'tweet'] = ''.join(tweet.split(',')[1:])\n","\n","# Processing\n","#test_df['tweet'] = test_df['tweet'].apply(lambda x: hlp.remove_stopwords(x))\n","#test_df['tweet'] = test_df['tweet'].apply(lambda x: hlp.remove_punct(x))\n","test_df['tweet'] = test_df['tweet'].apply(lambda x: hlp.add_space(x))\n","test_df['tweet'] = test_df['tweet'].apply(lambda x: hlp.remove_white_space(x))\n","#test_df['tweet'] = test_df['tweet'].apply(lambda x: hlp.remove_words_digits(x))\n","test_df['tweet'] = test_df['tweet'].apply(lambda x: hlp.to_lower(x))\n","#test_df['tweet'] = test_df['tweet'].apply(lambda x: hlp.remove_specific_words(x))\n","test_df['tweet'] = test_df['tweet'].apply(lambda x: hlp.remove_repeating_char(x))\n","test_df['tweet'] = test_df['tweet'].apply(lambda x: hlp.lemmatize(x))\n","test_df.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"slh6xmfUNAe0"},"outputs":[],"source":["class TweetTestDataset(Dataset):\n","  def __init__(self,tweets_df, tokenizer, max_len):\n","    super().__init__()\n","    self.tweets = tweets_df.tweet.to_numpy()\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","  \n","  def __len__(self):\n","    return len(self.tweets)\n","  \n","  def __getitem__(self, item):\n","    tweets = str(self.tweets[item])\n","    encoding = self.tokenizer.encode_plus(\n","      tweets,\n","      add_special_tokens=True,\n","      max_length=self.max_len,\n","      return_token_type_ids=False,\n","      pad_to_max_length=True,\n","      return_attention_mask=True,\n","      return_tensors='pt',\n","    )\n","    return {\n","      'tweet_text': tweets,\n","      'input_ids': encoding['input_ids'].flatten(),\n","      'attention_mask': encoding['attention_mask'].flatten(),\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NX_n3vrtNIE4"},"outputs":[],"source":["test_dataloader = DataLoader(TweetTestDataset(test_df,tokenizer, TOKENIZER_MAX_LEN))\n","model = SentimentClassifier.load_from_checkpoint(\"logs/checkpoints/last.ckpt\")\n","preds = trainer.predict(model, test_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9dQPE6OgPU9l"},"outputs":[],"source":["test_preds = np.array([int(preds[i][1]) for i in range(len(preds))])\n","test_preds[test_preds == 0] = -1\n","test_preds = pd.DataFrame(test_preds)\n","test_preds.index += 1\n","test_preds.to_csv('test_preds.csv',header='Prediction')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"09bd16cc9d6e4c77bd98ab82bc93abf1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0cd4b4a0a38046f5a20c9ef542433f8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"13fab2a81b324a2a8c071160223b330f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"1ef645a992b24073add4dd1a8b6fedb3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"295089d4cd2f44a19a65e5a7db89726d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_868bf2b6d7704d0a9fbb6a95e104dcc1","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fe8fa82ece694ed48a0532ff04d34c59","value":2}},"2dd5c9ef87f44dd89b4bbd739695fa55":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ccde8064e8504460aabe9c5b9c10d893","IPY_MODEL_e01e9b9b6f374433945244e839e8d306","IPY_MODEL_c3c55c7e8d124694a54b8a12c92f714c"],"layout":"IPY_MODEL_3ceb27d19523438ba36be11f4bfb2f5c"}},"3ceb27d19523438ba36be11f4bfb2f5c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"3d843c1ea5cd488eb3f42c75307a5c9d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40a50d8140d445bcaa482cea8415dc95":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6552ae43a46543b8aec6cf9f8a44760c","IPY_MODEL_d8104f8bda9247cc8bfbda69372dd082","IPY_MODEL_83dbb6105c1b4566b34912beb7d6a81e"],"layout":"IPY_MODEL_56c53c712f6943daa30e114e9e8b6576"}},"44459ba678f54c6191c614d007f7c2bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4954a08985774034a19b73dfcb550f5d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4b725bd619b64c51bfafacca14b12f3b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51258d65aabb4e6a8d7a176c8e58a293":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56c53c712f6943daa30e114e9e8b6576":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"5e44b32d5fb04ab1a254f3c40238b077":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6552ae43a46543b8aec6cf9f8a44760c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d3d69f5e2c64c5c8c820882fad26a40","placeholder":"​","style":"IPY_MODEL_51258d65aabb4e6a8d7a176c8e58a293","value":"Epoch 0:  24%"}},"6d3d69f5e2c64c5c8c820882fad26a40":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83dbb6105c1b4566b34912beb7d6a81e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8ca78a03fde4be4b9d1e06509f214dd","placeholder":"​","style":"IPY_MODEL_e6a5948d6a8a4586a778823823bffdec","value":" 1050/4310 [18:04&lt;56:06,  1.03s/it, loss=0.419, v_num=botk]"}},"868bf2b6d7704d0a9fbb6a95e104dcc1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a909c7e0f61641ebad29ec04e0e275ab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"acb1e4bd9ad74a729b0d5c2f9220e0cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b8e51e0768f4464082657a635acfaeb4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dbf1c0a737e14eae821bb920c075ff40","IPY_MODEL_295089d4cd2f44a19a65e5a7db89726d","IPY_MODEL_e00c539f56dd4a06a976a483e1aef78a"],"layout":"IPY_MODEL_13fab2a81b324a2a8c071160223b330f"}},"c3c55c7e8d124694a54b8a12c92f714c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cda2b46b5af44aa2893e1e2f8803f882","placeholder":"​","style":"IPY_MODEL_3d843c1ea5cd488eb3f42c75307a5c9d","value":" 308/308 [02:24&lt;00:00,  2.12it/s]"}},"ccde8064e8504460aabe9c5b9c10d893":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09bd16cc9d6e4c77bd98ab82bc93abf1","placeholder":"​","style":"IPY_MODEL_1ef645a992b24073add4dd1a8b6fedb3","value":"Validation DataLoader 0: 100%"}},"cda2b46b5af44aa2893e1e2f8803f882":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8104f8bda9247cc8bfbda69372dd082":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_a909c7e0f61641ebad29ec04e0e275ab","max":4310,"min":0,"orientation":"horizontal","style":"IPY_MODEL_acb1e4bd9ad74a729b0d5c2f9220e0cf","value":1050}},"dbf1c0a737e14eae821bb920c075ff40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e44b32d5fb04ab1a254f3c40238b077","placeholder":"​","style":"IPY_MODEL_f4029d0de91e4744a644a158601c2804","value":"Sanity Checking DataLoader 0: 100%"}},"e00c539f56dd4a06a976a483e1aef78a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b725bd619b64c51bfafacca14b12f3b","placeholder":"​","style":"IPY_MODEL_4954a08985774034a19b73dfcb550f5d","value":" 2/2 [00:02&lt;00:00,  1.15s/it]"}},"e01e9b9b6f374433945244e839e8d306":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_44459ba678f54c6191c614d007f7c2bd","max":308,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0cd4b4a0a38046f5a20c9ef542433f8f","value":308}},"e6a5948d6a8a4586a778823823bffdec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8ca78a03fde4be4b9d1e06509f214dd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4029d0de91e4744a644a158601c2804":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe8fa82ece694ed48a0532ff04d34c59":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
